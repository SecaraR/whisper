{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35a99588-94a7-49d7-85fe-2d98b7d6b1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/lobby/KINGSTON/.cache/huggingface'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/media/lobby/KINGSTON/.cache/huggingface'\n",
    "os.environ['HF_HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "651e9c60-b29d-49a5-aba1-7a68cfb92d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3829ead0-4597-4bf7-b5cc-5b970e1c7712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('input/Doc1-page1_part_1.jpeg', 'output/Doc1-page1_part_1.txt')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file_name = \"Doc1-page1\"\n",
    "file_name = \"Doc1-page1_part_1\"\n",
    "# file_name = \"Doc1-page1-resized\"\n",
    "# file_name = \"google\"\n",
    "# file_name = \"japanese\"\n",
    "\n",
    "input_file_ext = \"jpeg\"\n",
    "# input_file_ext = \"jpg\"\n",
    "# input_file_ext = \"png\"\n",
    "\n",
    "# input_path = r\"a.txt\"\n",
    "input_path = os.path.join(\"input\", f\"{file_name}.{input_file_ext}\")\n",
    "# input_path = os.path.join(\"input\", f\"{file_name}.png\")\n",
    "\n",
    "# output_path = \"/content/aa.txt\"\n",
    "output_path = os.path.join(\"output\", f\"{file_name}.txt\")\n",
    "input_path, output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "211befd8-f620-4a22-98a8-d788700ccd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed8cde78be34210a17c8a11267929d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "# model_name = \"\"\n",
    "\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=\"auto\", \n",
    "    device_map=\"auto\",\n",
    "    # attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db40f0cf-b64e-4069-b07d-ec007c0609cd",
   "metadata": {},
   "source": [
    "Content examples:\n",
    "\n",
    "```json\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\":f\"/media/lobby/KINGSTON/GitHub-SecaraR/whisper/notebooks/image-2-text/Qwen2-VL/{input_path}\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e588cd1-7102-4d99-b642-c36b32b769cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': [{'type': 'image',\n",
       "    'image': '/media/lobby/KINGSTON/GitHub-SecaraR/whisper/notebooks/image-2-text/Qwen2-VL/input/Doc1-page1_part_1.jpeg'},\n",
       "   {'type': 'text', 'text': 'Output all the text from this image.'}]}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\":f\"/media/lobby/KINGSTON/GitHub-SecaraR/whisper/notebooks/image-2-text/Qwen2-VL/{input_path}\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Output all the text from this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15aeee1f-5fd1-442c-be7c-53eae59ff48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output/Doc1-page1_part_1.txt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "# inputs = inputs.to(\"cpu\")\n",
    "# inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "# clear file content\n",
    "open(output_path, 'w').close()\n",
    "\n",
    "# add content to file\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    f.writelines(output_text)\n",
    "\n",
    "output_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

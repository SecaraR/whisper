{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a99588-94a7-49d7-85fe-2d98b7d6b1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/media/lobby/KINGSTON/.cache/huggingface'\n",
    "os.environ['HF_HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651e9c60-b29d-49a5-aba1-7a68cfb92d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211befd8-f620-4a22-98a8-d788700ccd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(r\"facebook/nllb-200-distilled-600M\", token=True, src_lang=\"ja\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(r\"facebook/nllb-200-distilled-600M\", token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3829ead0-4597-4bf7-b5cc-5b970e1c7712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_path = r\"a.txt\"\n",
    "input_path = os.path.join(\"input\", f\"a.txt\")\n",
    "\n",
    "# output_path = \"/content/aa.txt\"\n",
    "output_path = os.path.join(\"output\", f\"aa.txt\")\n",
    "input_path, output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231ba042-d50a-498e-a7c5-4c330e4e1305",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = open(input_path, 'r', encoding='utf-8')\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for article in input_file:\n",
    "        inputs = tokenizer(article, return_tensors=\"pt\")\n",
    "    \n",
    "        print(article)\n",
    "        # print(inputs)\n",
    "        \n",
    "        # Access the 'input_ids' tensor from the 'inputs' dictionary\n",
    "        translated_tokens = model.generate(\n",
    "            inputs['input_ids'],  # Pass the tensor to model.generate()\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"eng_Latn\"),\n",
    "            max_length=512\n",
    "        )\n",
    "        # print(tokenizer.convert_tokens_to_ids(\"eng_Latn\"))\n",
    "        \n",
    "        output = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True, model_max_length=512)[0]\n",
    "        \n",
    "        # print(output)\n",
    "        f.writelines(output + '\\n')\n",
    "\n",
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c517626-70fb-45d3-a6cb-d49879e1d169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "pipe = pipeline(\"text-generation\", model=\"lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese\")\n",
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-30 22:36:49 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 03-30 22:36:56 [config.py:585] This model supports multiple tasks: {'embed', 'classify', 'score', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 03-30 22:36:56 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 03-30 22:36:57 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese', speculative_config=None, tokenizer='lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 03-30 22:36:58 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7a9bab02ad70>\n",
      "INFO 03-30 22:36:58 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 03-30 22:36:58 [cuda.py:220] Using Flash Attention backend on V1 engine.\n",
      "INFO 03-30 22:36:58 [gpu_model_runner.py:1174] Starting to load model lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese...\n",
      "WARNING 03-30 22:36:58 [config.py:3692] `torch.compile` is turned on, but the model lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese does not support it. Please open an issue on GitHub if you want it to be supported.\n",
      "ERROR 03-30 22:36:58 [core.py:343] EngineCore hit an exception: Traceback (most recent call last):\n",
      "ERROR 03-30 22:36:58 [core.py:343]   File \"/home/lobby/venvs/whisper-env/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 335, in run_engine_core\n",
      "ERROR 03-30 22:36:58 [core.py:343]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "ERROR 03-30 22:36:58 [core.py:343]   File \"/home/lobby/venvs/whisper-env/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 290, in __init__\n",
      "ERROR 03-30 22:36:58 [core.py:343]     super().__init__(vllm_config, executor_class, log_stats)\n",
      "ERROR 03-30 22:36:58 [core.py:343]   File \"/home/lobby/venvs/whisper-env/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 60, in __init__\n",
      "ERROR 03-30 22:36:58 [core.py:343]     self.model_executor = executor_class(vllm_config)\n",
      "ERROR 03-30 22:36:58 [core.py:343]   File \"/home/lobby/venvs/whisper-env/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 52, in __init__\n",
      "ERROR 03-30 22:36:58 [core.py:343]     self._init_executor()\n",
      "ERROR 03-30 22:36:58 [core.py:343]   File \"/home/lobby/venvs/whisper-env/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "ERROR 03-30 22:36:58 [core.py:343]     self.collective_rpc(\"load_model\")\n",
      "ERROR 03-30 22:36:58 [core.py:343]   File \"/home/lobby/venvs/whisper-env/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "ERROR 03-30 22:36:58 [core.py:343]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "ERROR 03-30 22:36:58 [core.py:343]   File \"/home/lobby/venvs/whisper-env/lib/python3.10/site-packages/vllm/utils.py\", line 2255, in run_method\n",
      "ERROR 03-30 22:36:58 [core.py:343]     return func(*args, **kwargs)\n",
      "ERROR 03-30 22:36:58 [core.py:343]   File \"/home/lobby/venvs/whisper-env/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 136, in load_model\n",
      "ERROR 03-30 22:36:58 [core.py:343]     self.model_runner.load_model()\n",
      "ERROR 03-30 22:36:58 [core.py:343]   File \"/home/lobby/venvs/whisper-env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1177, in load_model\n",
      "ERROR 03-30 22:36:58 [core.py:343]     self.model = get_model(vllm_config=self.vllm_config)\n",
      "ERROR 03-30 22:36:58 [core.py:343]   File \"/home/lobby/venvs/whisper-env/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n",
      "ERROR 03-30 22:36:58 [core.py:343]     return loader.load_model(vllm_config=vllm_config)\n",
      "ERROR 03-30 22:36:58 [core.py:343]   File \"/home/lobby/venvs/whisper-env/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 441, in load_model\n",
      "ERROR 03-30 22:36:58 [core.py:343]     model = _initialize_model(vllm_config=vllm_config)\n",
      "ERROR 03-30 22:36:58 [core.py:343]   File \"/home/lobby/venvs/whisper-env/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 127, in _initialize_model\n",
      "ERROR 03-30 22:36:58 [core.py:343]     return model_class(vllm_config=vllm_config, prefix=prefix)\n",
      "ERROR 03-30 22:36:58 [core.py:343]   File \"/home/lobby/venvs/whisper-env/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 431, in __init__\n",
      "ERROR 03-30 22:36:58 [core.py:343]     self.model = Qwen2Model(vllm_config=vllm_config,\n",
      "ERROR 03-30 22:36:58 [core.py:343]   File \"/home/lobby/venvs/whisper-env/lib/python3.10/site-packages/vllm/compilation/decorators.py\", line 151, in __init__\n",
      "ERROR 03-30 22:36:58 [core.py:343]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "ERROR 03-30 22:36:58 [core.py:343]   File \"/home/lobby/venvs/whisper-env/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 300, in __init__\n",
      "ERROR 03-30 22:36:58 [core.py:343]     self.start_layer, self.end_layer, self.layers = make_layers(\n",
      "ERROR 03-30 22:36:58 [core.py:343]   File \"/home/lobby/venvs/whisper-env/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 567, in make_layers\n",
      "ERROR 03-30 22:36:58 [core.py:343]     [PPMissingLayer() for _ in range(start_layer)] + [\n",
      "ERROR 03-30 22:36:58 [core.py:343]   File \"/home/lobby/venvs/whisper-env/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 568, in <listcomp>\n",
      "ERROR 03-30 22:36:58 [core.py:343]     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n",
      "ERROR 03-30 22:36:58 [core.py:343]   File \"/home/lobby/venvs/whisper-env/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 302, in <lambda>\n",
      "ERROR 03-30 22:36:58 [core.py:343]     lambda prefix: Qwen2DecoderLayer(config=config,\n",
      "ERROR 03-30 22:36:58 [core.py:343]   File \"/home/lobby/venvs/whisper-env/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 218, in __init__\n",
      "ERROR 03-30 22:36:58 [core.py:343]     self.mlp = Qwen2MLP(\n",
      "ERROR 03-30 22:36:58 [core.py:343]   File \"/home/lobby/venvs/whisper-env/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 75, in __init__\n",
      "ERROR 03-30 22:36:58 [core.py:343]     self.gate_up_proj = MergedColumnParallelLinear(\n",
      "ERROR 03-30 22:36:58 [core.py:343]   File \"/home/lobby/venvs/whisper-env/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 533, in __init__\n",
      "ERROR 03-30 22:36:58 [core.py:343]     super().__init__(input_size=input_size,\n",
      "ERROR 03-30 22:36:58 [core.py:343]   File \"/home/lobby/venvs/whisper-env/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 398, in __init__\n",
      "ERROR 03-30 22:36:58 [core.py:343]     self.quant_method.create_weights(\n",
      "ERROR 03-30 22:36:58 [core.py:343]   File \"/home/lobby/venvs/whisper-env/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 178, in create_weights\n",
      "ERROR 03-30 22:36:58 [core.py:343]     weight = Parameter(torch.empty(sum(output_partition_sizes),\n",
      "ERROR 03-30 22:36:58 [core.py:343]   File \"/home/lobby/venvs/whisper-env/lib/python3.10/site-packages/torch/utils/_device.py\", line 104, in __torch_function__\n",
      "ERROR 03-30 22:36:58 [core.py:343]     return func(*args, **kwargs)\n",
      "ERROR 03-30 22:36:58 [core.py:343] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 5.79 GiB of which 101.00 MiB is free. Including non-PyTorch memory, this process has 5.63 GiB memory in use. Of the allocated memory 5.50 GiB is allocated by PyTorch, and 11.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "ERROR 03-30 22:36:58 [core.py:343] \n",
      "CRITICAL 03-30 22:36:58 [core_client.py:269] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese\",\n",
    "    max_model_len=8_000\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.5, \n",
    "    max_tokens=8_000,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    \"\"\"学校には1クラスにつき20人の生徒がおり、クラスは合計3つあります。\n",
    "学校全体では男子と女子がそれぞれ50%ずついます。\n",
    "1つ目のクラスには女子が15人、2つ目のクラスには女子が12人います。\n",
    "3つ目のクラスには何人の男子がいますか？\"\"\"\n",
    "]\n",
    "\n",
    "conversations = [\n",
    "    [{\"role\": \"user\", \"content\": x}] for x in prompts\n",
    "]\n",
    "\n",
    "outputs = llm.chat(conversations, sampling_params=sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    print(output.outputs[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
